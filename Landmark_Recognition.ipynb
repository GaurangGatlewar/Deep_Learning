{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "from skimage.feature import plot_matches\n",
    "from skimage.measure import ransac\n",
    "from skimage.transform import AffineTransform\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.platform import app\n",
    "from delf import feature_io\n",
    "\n",
    "cmd_args = None\n",
    "\n",
    "_DISTANCE_THRESHOLD = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Setup for recognition"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The setup part needs to be run only the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train list based on hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_with_hash.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list = [(row[6],row[4]) for row in train.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tl = pd.DataFrame(train_list,columns=['Hash','landmark_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tl.to_csv(\"train_list.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test list based on hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test_with_hash.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list = [(row[4],row[2]) for row in test.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list[5000:5010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_list = pd.DataFrame(test_list,columns=['Hash','id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_list.to_csv(\"test_list.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delf for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to compare 2 files using DELF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delf_compare(file1,file2):\n",
    "    try:\n",
    "        # Read features.\n",
    "        locations_1, _, descriptors_1, _, _ = feature_io.ReadFromFile(file1)\n",
    "        num_features_1 = locations_1.shape[0]\n",
    "        locations_2, _, descriptors_2, _, _ = feature_io.ReadFromFile(file2)\n",
    "        num_features_2 = locations_2.shape[0]\n",
    "\n",
    "        # Find nearest-neighbor matches using a KD tree.\n",
    "        d1_tree = cKDTree(descriptors_1)\n",
    "        _, indices = d1_tree.query(\n",
    "          descriptors_2, distance_upper_bound=_DISTANCE_THRESHOLD)\n",
    "\n",
    "        # Select feature locations for putative matches.\n",
    "        locations_2_to_use = np.array([\n",
    "          locations_2[i,]\n",
    "          for i in range(num_features_2)\n",
    "          if indices[i] != num_features_1\n",
    "        ])\n",
    "        locations_1_to_use = np.array([\n",
    "          locations_1[indices[i],]\n",
    "          for i in range(num_features_2)\n",
    "          if indices[i] != num_features_1\n",
    "        ])\n",
    "\n",
    "        # Perform geometric verification using RANSAC.\n",
    "        _, inliers = ransac(\n",
    "          (locations_1_to_use, locations_2_to_use),\n",
    "          AffineTransform,\n",
    "          min_samples=3,\n",
    "          residual_threshold=20,\n",
    "          max_trials=1000)\n",
    "\n",
    "        answer = sum(inliers)\n",
    "        confidence = (2*answer)/(num_features_1+num_features_2)\n",
    "        #print ('Of %d and %d ,found %d inliers' % (num_features_1,num_features_2,answer))\n",
    "        return (answer,confidence)\n",
    "    except:\n",
    "        return (0,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"train_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list = df1[['Hash','landmark_id']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"test_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list = df2[['Hash','id']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Landmark Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_delf = \"/beegfs/ss8464/code/data/query_features/\"\n",
    "train_delf = \"/beegfs/ss8464/code/train_features/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_with_hash.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (os.path.isfile(\"test_landmarks.csv\")):\n",
    "    landmarks = pd.read_csv(\"test_landmarks.csv\",names = [\"index\",\"id\",\"landmarks\"])\n",
    "    del landmarks[\"index\"]\n",
    "    landmarks = landmarks[1:]\n",
    "    test_landmarks = landmarks[[\"id\",\"landmarks\"]].values.tolist()\n",
    "else:\n",
    "    test_landmarks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the closest estimates for an image and determining the landmark class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_landmark(Hash,Id,pos):\n",
    "    while(Hash>train_list[pos][0]):\n",
    "        pos += 1\n",
    "    d = {}\n",
    "    for x in range(1,min(1000,pos)):\n",
    "        h,i = train_list[pos-x]\n",
    "        if(i in d.keys()):\n",
    "            d[i] += (1/x)\n",
    "        else:\n",
    "            d[i] = (1/x)\n",
    "    for x in range(1,min(1000,len(train_list)-pos)):\n",
    "        h,i = train_list[pos+x]\n",
    "        if(i in d.keys()):\n",
    "            d[i] += (1/x)\n",
    "        else:\n",
    "            d[i] = (1/x)\n",
    "    temp = [(d[i],i) for i in d.keys()]\n",
    "    temp.sort()\n",
    "    temp = temp[:10]\n",
    "    mxval = 0\n",
    "    mxcon = 0.01\n",
    "    for val,lm in temp:\n",
    "        Id1 = train[train['landmark_id']==lm]['id'].iloc[0]\n",
    "        check,confidence = delf_compare(test_delf+Id+\".delf\",train_delf+Id1+\".delf\")\n",
    "        if(check>=20):\n",
    "            return (str(lm)+\" \"+str(confidence),pos)\n",
    "        if(check>mxval):\n",
    "            mxval = check\n",
    "            answer = str(lm)+\" \"+str(confidence)\n",
    "        if(mxval<5):\n",
    "            answer = \"-1 0.01\"\n",
    "    return (answer,pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the test images classified using DELF features in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = 0\n",
    "start = time.time()\n",
    "for i in range(len(test_landmarks),len(test_list)):\n",
    "    if(i%100==0):\n",
    "        landmarks = pd.DataFrame(test_landmarks,columns=[\"id\",\"landmarks\"])\n",
    "        landmarks.to_csv(\"test_landmarks.csv\")\n",
    "        print (time.time()-start)\n",
    "    Hash,Id = test_list[i]\n",
    "    if (not os.path.isfile(test_delf+str(Id)+\".delf\")):\n",
    "        test_landmarks.append([Id,-2])\n",
    "        continue\n",
    "    H_prev,Id_prev = test_list[i-1]\n",
    "    try:\n",
    "        check,confidence = delf_compare(test_delf+str(Id)+\".delf\",test_delf+str(Id_prev)+\".delf\")\n",
    "        if(check>15):\n",
    "            test_landmarks.append([Id,str(test_landmarks[-1][1])+\" \"+str(confidence)])\n",
    "        else:\n",
    "            landmark,pos = get_landmark(Hash,Id,pos)\n",
    "            test_landmarks.append([Id,landmark])\n",
    "    except:\n",
    "        landmark,pos = get_landmark(Hash,Id,pos)\n",
    "        test_landmarks.append([Id,landmark])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
