{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Major defines\n",
    "N_INPUTS  = 32*32*3\n",
    "N_CLASSES = 10\n",
    "BATCH_SIZE = 64\n",
    "data_path_train = 'train_old/'\n",
    "data_path_labels = 'labels_old.txt'\n",
    "data_path_test = 'test_old/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "# Generating training data\n",
    "filenames = [data_path_train+file for file in os.listdir(data_path_train)]  #get all filenames\n",
    "random.shuffle(filenames)\n",
    "labels = [file.split('_')[-1].split('.')[0] for file in filenames ] #get label string name eg. frog, ship...\n",
    "#labels = list(map(int,labels))\n",
    "\n",
    "id2label, label2id = get_label_mapping(data_path_labels) \n",
    "labels = [label2id[label] for label in labels if label in label2id] #map label  name to id number eg  ship -> 8\n",
    "\n",
    "train_size = int(((len(filenames)*0.95)//BATCH_SIZE)*BATCH_SIZE)\n",
    "val_size = int(((len(filenames)*0.05)//BATCH_SIZE)*BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create input dataset and generate batches of data\n",
    "def _parse_function(filenames, labels):\n",
    "#     one_hot = tf.one_hot(labels, N_CLASSES)   #because using sparse cross entropy\n",
    "    \n",
    "    img_string = tf.read_file(filenames)\n",
    "    img_decoded = tf.image.decode_png(img_string, channels=3)\n",
    "    \n",
    "    #Reserved for image augmentation\n",
    "    float_image = tf.image.per_image_standardization(img_decoded)\n",
    "    \n",
    "    \n",
    "#     float_image = tf.image.random_flip_left_right(float_image, seed = 1)\n",
    "#     float_image = tf.contrib.image.rotate(float_image, np.random.randint(-15,16)) \n",
    "#     float_image = tf.image.random_brightness(float_image, 0.3)\n",
    "#     float_image = tf.image.random_contrast(float_image, 0.1, 0.9)\n",
    "#     float_image = tf.image.random_saturation(float_image, 0.1, 0.9)\n",
    "    \n",
    "    img_decoded = tf.reshape(float_image , [-1])  #flatten\n",
    "    return img_decoded, labels\n",
    "\n",
    "# Train data\n",
    "data = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "data = data.map(_parse_function, num_parallel_calls = 3)\n",
    "data = data.batch(BATCH_SIZE)\n",
    "data = data.shuffle(buffer_size=2000)    #random number \n",
    "iterator = data.make_initializable_iterator()\n",
    "train_batch = iterator.get_next()  #next_element is tensor of (img_train, y_train)\n",
    "\n",
    "# Validation data\n",
    "if(len(filenames[train_size:(train_size+val_size)])>0):\n",
    "    val = tf.data.Dataset.from_tensor_slices((filenames[train_size:(train_size+val_size)], labels[train_size:(train_size+val_size)]))   #validation batchval = tf.data.Dataset.from_tensor_slices((filenames[train_size:(train_size+val_size)], labels[train_size:(train_size+val_size)]))   #validation batch\n",
    "else:\n",
    "    val = tf.data.Dataset.from_tensor_slices((filenames[-1*BATCH_SIZE:], labels[-1*BATCH_SIZE:]))\n",
    "val = val.map(_parse_function, num_parallel_calls = 3)\n",
    "val = val.batch(5000)\n",
    "val_iterator = val.make_initializable_iterator()\n",
    "validation_batch = val_iterator.get_next()\n",
    "\n",
    "\n",
    "\n",
    "# For test images\n",
    "def _parse_function_test(filenames):\n",
    "    img_string = tf.read_file(filenames)\n",
    "    img_decoded = tf.image.decode_png(img_string, channels=3)\n",
    "    \n",
    "    #Reserved for image augmentation\n",
    "    float_image = tf.image.per_image_standardization(img_decoded)\n",
    "    img_decoded = tf.reshape(float_image , [-1])  #flatten\n",
    "    \n",
    "    return img_decoded\n",
    "\n",
    "filenames_test = [data_path_test+file_t for file_t in os.listdir(data_path_test)]\n",
    "filenames_test.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "\n",
    "\n",
    "#For test data\n",
    "data_test = tf.data.Dataset.from_tensor_slices(filenames_test)\n",
    "data_test = data_test.map(_parse_function_test)\n",
    "data_test = data_test.batch(2000)\n",
    "test_iterator = data_test.make_initializable_iterator()\n",
    "X_t = test_iterator.get_next() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_model(X, N_CLASSES, reuse, is_training):\n",
    "    \n",
    "    with tf.variable_scope('Conv', reuse = reuse): #to reuse weights and biases for testing\n",
    "        \n",
    "        input_layer = tf.reshape(X, [-1, 32, 32, 3])\n",
    "\n",
    "        conv1_1 = tf.layers.conv2d(\n",
    "          inputs=input_layer,\n",
    "          filters=64,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "        \n",
    "        conv1_2 = tf.layers.conv2d(\n",
    "          inputs=conv1_1,\n",
    "          filters=64,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu) \n",
    "       \n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            inputs = conv1_2,\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding = \"same\",\n",
    "            data_format='channels_last')\n",
    "        \n",
    "        conv2_1 = tf.layers.conv2d(\n",
    "          inputs=pool1,\n",
    "          filters=128,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "        \n",
    "        conv2_2 = tf.layers.conv2d(\n",
    "          inputs=conv2_1,\n",
    "          filters=128,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu) \n",
    "       \n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            inputs = conv2_2,\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding = \"same\",\n",
    "            data_format='channels_last')\n",
    "        \n",
    "        conv3_1 = tf.layers.conv2d(\n",
    "          inputs=pool2,\n",
    "          filters=256,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "        \n",
    "        conv3_2 = tf.layers.conv2d(\n",
    "          inputs=conv3_1,\n",
    "          filters=256,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "        \n",
    "        conv3_3 = tf.layers.conv2d(\n",
    "          inputs=conv3_2,\n",
    "          filters=256,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu) \n",
    "       \n",
    "        pool3 = tf.layers.max_pooling2d(\n",
    "            inputs = conv3_3,\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding = \"same\",\n",
    "            data_format='channels_last')\n",
    "        \n",
    "        conv4_1 = tf.layers.conv2d(\n",
    "          inputs=pool3,\n",
    "          filters=512,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "        \n",
    "        conv4_2 = tf.layers.conv2d(\n",
    "          inputs=conv4_1,\n",
    "          filters=512,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "        \n",
    "        conv4_3 = tf.layers.conv2d(\n",
    "          inputs=conv4_2,\n",
    "          filters=512,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu) \n",
    "       \n",
    "        pool4 = tf.layers.max_pooling2d(\n",
    "            inputs = conv4_3,\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding = \"same\",\n",
    "            data_format='channels_last')\n",
    "        \n",
    "        conv5_1 = tf.layers.conv2d(\n",
    "          inputs=pool4,\n",
    "          filters=512,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "        \n",
    "        conv5_2 = tf.layers.conv2d(\n",
    "          inputs=conv5_1,\n",
    "          filters=512,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "        \n",
    "        conv5_3 = tf.layers.conv2d(\n",
    "          inputs=conv5_2,\n",
    "          filters=512,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu) \n",
    "       \n",
    "        pool5 = tf.layers.max_pooling2d(\n",
    "            inputs = conv5_3,\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding = \"same\",\n",
    "            data_format='channels_last')\n",
    "        \n",
    "        flatten = tf.layers.flatten(pool5)\n",
    "\n",
    "        dense1 = tf.layers.dense(\n",
    "            inputs = flatten,\n",
    "            units = 1024,\n",
    "            activation= tf.nn.relu)\n",
    "\n",
    "#         dropout2 = tf.layers.dropout(\n",
    "#             inputs = dense1,\n",
    "#             rate=0.25,\n",
    "#             training = is_training)\n",
    "\n",
    "        dense2 = tf.layers.dense(\n",
    "            inputs = dense1,\n",
    "            units = 1024,\n",
    "            activation= tf.nn.relu)\n",
    "        \n",
    "        dense3 = tf.layers.dense(\n",
    "            inputs = dense2,\n",
    "            units = N_CLASSES)\n",
    "        \n",
    "        if is_training:\n",
    "            last_layer = dense3     #using sparse cross entropy so no need to apply softmax here\n",
    "        else:\n",
    "            last_layer = tf.nn.softmax(dense3)   #for inference\n",
    "\n",
    "        #FOR FUTURE\n",
    "    #     predictions = {\n",
    "    #       # Generate predictions (for PREDICT and EVAL mode)\n",
    "    #       \"classes\": tf.argmax(input=logits, axis=1),\n",
    "    #       # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "    #       # `logging_hook`.\n",
    "    #       \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    #       }\n",
    "\n",
    "    #     if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    #         return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "        return last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = train_batch\n",
    "\n",
    "valX, valY = validation_batch\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable = False, name='global_step')\n",
    "\n",
    "train_output = conv_model(X, N_CLASSES, reuse = False, is_training = True)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits (labels = y, logits = train_output))\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost, global_step = global_step)\n",
    "\n",
    "#NOTE: THIS IS VERY INEFFICIENT. IMPROVE THIS BY USING feed_dict\n",
    "\n",
    "# Evaluate model with train data\n",
    "test_output = conv_model(X, N_CLASSES, reuse=True, is_training=False)\n",
    "correct_pred = tf.equal(tf.argmax(test_output, 1, output_type=tf.int32), y)\n",
    "train_accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# # Evaluate model with Validation data\n",
    "val_test_output = conv_model(valX, N_CLASSES, reuse=True, is_training=False)\n",
    "val_pred = tf.equal(tf.argmax(val_test_output, 1, output_type=tf.int32), valY)\n",
    "val_accuracy = tf.reduce_mean(tf.cast(val_pred, tf.float32))\n",
    "\n",
    "#for test data\n",
    "test_r = conv_model(X_t, N_CLASSES, reuse=True, is_training=False)\n",
    "test_pred = tf.argmax(test_r, 1 , output_type=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1    Loss: 2.3046    Train Accuracy: 0.06    Val Accuracy: 0.11    Time: 96.27\n",
      "Epoch 2    Loss: 2.3039    Train Accuracy: 0.11    Val Accuracy: 0.10    Time: 70.61\n",
      "Epoch 3    Loss: 2.3022    Train Accuracy: 0.08    Val Accuracy: 0.10    Time: 70.55\n",
      "Epoch 4    Loss: 2.3008    Train Accuracy: 0.14    Val Accuracy: 0.10    Time: 70.68\n",
      "Done Training\n",
      "Model saved at checkpoints/GLRC-2813\n",
      "Writing complete\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(init)   #initialize variables\n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    \n",
    "    epochCount = 1\n",
    "        \n",
    "    while (epochCount < N_EPOCHS):\n",
    "        startTime = time.time()\n",
    "        while True:\n",
    "            try:\n",
    "                sess.run(optimizer)\n",
    "                \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                sess.run(iterator.initializer)\n",
    "                tr_loss, tr_acc = sess.run([cost, train_accuracy])\n",
    "                \n",
    "                sess.run(val_iterator.initializer)\n",
    "                val_acc = sess.run(val_accuracy)\n",
    "                print(\"Epoch {}    Loss: {:,.4f}    Train Accuracy: {:,.2f}    Val Accuracy: {:,.2f}    Time: {:,.2f}\"         \n",
    "                      .format(epochCount, tr_loss, tr_acc, val_acc, time.time() - startTime))\n",
    "                \n",
    "               \n",
    "                epochCount += 1\n",
    "                break\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print ('\\nTraining Interrupted at epoch %d' % epochCount)\n",
    "            \n",
    "                epochCount = N_EPOCHS + 1\n",
    "                break\n",
    "    print ('Done Training')\n",
    "    #Save the model\n",
    "    save_path = saver.save(sess, 'checkpoints/GLRC', global_step = global_step )\n",
    "    print ('Model saved at %s' % save_path)\n",
    "    \n",
    "    sess.run(test_iterator.initializer)\n",
    "    test_predictions = []\n",
    "    while True:\n",
    "        try:\n",
    "            test_predictions.extend(sess.run(test_pred))\n",
    "            \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print (\"Prediction interrupted\")\n",
    "            break\n",
    "            \n",
    "        except:\n",
    "            print ()\n",
    "            break\n",
    "            \n",
    "    predictions_csv = open('GLRC_'+str(epochCount)+'.csv', 'w')\n",
    "    header = ['id','label']\n",
    "    with predictions_csv:\n",
    "        writer = csv.writer(predictions_csv)\n",
    "        writer.writerow((header[0], header[1]))\n",
    "        for count, row in enumerate(range(len(test_predictions))):\n",
    "            writer.writerow((count, test_predictions[count]))\n",
    "        print(\"Writing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
